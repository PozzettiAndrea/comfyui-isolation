# Wheel sources registry for CUDA packages
#
# Template variables:
#   {cuda_version}   - Full CUDA version (e.g., "12.8")
#   {cuda_short}     - CUDA without dot (e.g., "128")
#   {cuda_short2}    - CUDA short for spconv (e.g., "124" not "1240")
#   {cuda_major}     - CUDA major version (e.g., "12")
#   {torch_version}  - Full PyTorch version (e.g., "2.8.0")
#   {torch_short}    - PyTorch without dots (e.g., "280")
#   {torch_mm}       - PyTorch major.minor no dot (e.g., "28")
#   {torch_dotted_mm}- PyTorch major.minor with dot (e.g., "2.8")
#   {py_version}     - Python version (e.g., "3.10")
#   {py_short}       - Python without dot (e.g., "310")
#   {py_minor}       - Python minor version only (e.g., "10")
#   {py_tag}         - Python tag (e.g., "cp310")
#   {platform}       - Platform tag (e.g., "linux_x86_64")
#
# Install methods:
#   index          - pip --extra-index-url (PEP 503)
#   find_links     - pip --find-links
#   github_index   - GitHub Pages index (--find-links)
#   pypi_variant   - Package name varies by CUDA (e.g., spconv-cu124)
#   github_release - Direct wheel URL from GitHub releases
#   pypi           - Standard PyPI install

packages:
  # ===========================================================================
  # PyTorch Geometric (PyG) - official index
  # https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html
  # ===========================================================================
  torch-scatter:
    method: find_links
    index_url: "https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_short}.html"
    description: Scatter operations for PyTorch

  torch-cluster:
    method: find_links
    index_url: "https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_short}.html"
    description: Clustering algorithms for PyTorch

  torch-sparse:
    method: find_links
    index_url: "https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_short}.html"
    description: Sparse tensor operations for PyTorch

  torch-spline-conv:
    method: find_links
    index_url: "https://data.pyg.org/whl/torch-{torch_version}+cu{cuda_short}.html"
    description: Spline convolutions for PyTorch

  # ===========================================================================
  # pytorch3d - Facebook's official wheels
  # https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md
  # ===========================================================================
  pytorch3d:
    method: index
    index_url: "https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py3{py_minor}_cu{cuda_short}_pyt{torch_short}/download.html"
    description: PyTorch3D - 3D deep learning library

  # ===========================================================================
  # PozzettiAndrea cuda-wheels (unified PEP 503 index)
  # https://pozzettiandrea.github.io/cuda-wheels
  # ===========================================================================
  nvdiffrast:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: NVIDIA differentiable rasterizer

  cumesh:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: CUDA-accelerated mesh utilities

  o_voxel:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: O-Voxel CUDA extension for TRELLIS

  flex_gemm:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: Flexible GEMM operations

  nvdiffrec_render:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: NVDiffRec rendering utilities

  gsplat:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: Gaussian splatting rasterization

  cc_torch:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: GPU-accelerated connected components

  torch_generic_nms:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: GPU-accelerated Non-Maximum Suppression

  lietorch:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: Lie group operations for PyTorch (DPVO dependency)

  # ===========================================================================
  # spconv - PyPI with CUDA-versioned package names
  # ===========================================================================
  spconv:
    method: pypi_variant
    package_template: "spconv-cu{cuda_short2}"
    description: Sparse convolution library (use spconv-cu126 for CUDA 12.6+)

  # ===========================================================================
  # sageattention - Fast quantized attention
  # ===========================================================================
  sageattention:
    method: index
    index_url: "https://pozzettiandrea.github.io/cuda-wheels"
    description: SageAttention - 2-5x faster than FlashAttention with quantized kernels

  # ===========================================================================
  # triton - Required for sageattention on Linux
  # ===========================================================================
  triton:
    method: pypi
    description: Triton compiler for custom CUDA kernels

  # ===========================================================================
  # detectron2 - Facebook's detection library
  # https://github.com/facebookresearch/detectron2
  # Prebuilt wheels from miropsota's torch_packages_builder
  # Version format: 0.6+<commit>pt<torch_version>cu<cuda_short>
  # ===========================================================================
  detectron2:
    method: index
    index_url: "https://miropsota.github.io/torch_packages_builder"
    version_template: "0.6+2a420edpt{torch_version}cu{cuda_short}"
    description: Detectron2 - Facebook's detection and segmentation library

  # ===========================================================================
  # flash-attn - Multi-source prebuilt wheels
  # ===========================================================================
  flash-attn:
    method: github_release
    description: Flash Attention for fast transformer inference
    sources:
      - name: Dao-AILab
        url_template: "https://github.com/Dao-AILab/flash-attention/releases/download/v{version}/flash_attn-{version}%2Bcu{cuda_major}torch{torch_dotted_mm}cxx11abiTRUE-{py_tag}-{py_tag}-linux_x86_64.whl"
        platforms: [linux_x86_64]
      - name: mjun0812
        url_template: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.7/flash_attn-{version}%2Bcu{cuda_short}torch{torch_dotted_mm}-{py_tag}-{py_tag}-linux_x86_64.whl"
        platforms: [linux_x86_64]
      - name: mjun0812-windows
        url_template: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.7/flash_attn-{version}%2Bcu{cuda_short}torch{torch_dotted_mm}-{py_tag}-{py_tag}-win_amd64.whl"
        platforms: [win_amd64]
      - name: bdashore3
        url_template: "https://github.com/bdashore3/flash-attention/releases/download/v{version}/flash_attn-{version}%2Bcu{cuda_short}torch{torch_version}cxx11abiFALSE-{py_tag}-{py_tag}-win_amd64.whl"
        platforms: [win_amd64]
